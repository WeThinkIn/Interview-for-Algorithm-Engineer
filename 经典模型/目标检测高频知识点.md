# 目录

- [1.目标检测中IOU的相关概念与计算](#user-content-1.目标检测中iou的相关概念与计算)
- [2.目标检测中NMS的相关概念与计算](#user-content-2.目标检测中nms的相关概念与计算)
- [3.One-stage目标检测与Two-stage目标检测的区别？](#user-content-3.one-stage目标检测与two-stage目标检测的区别？)
- [4.哪些方法可以提升小目标检测的效果？](#user-content-4.哪些方法可以提升小目标检测的效果？)
- [5.Focal Loss的作用？](#user-content-5.focal-loss的作用？)
- [6.YOLO系列的面试问题](#user-content-6.yolo系列的面试问题)
- [7.有哪些经典的轻量型人脸检测模型？](#user-content-7.有哪些经典的轻量型人脸检测模型？)
- [8.LFFD人脸检测模型的结构和特点？](#user-content-8.lffd人脸检测模型的结构和特点？)
- [9.目标检测中AP，AP50，AP75，mAP等指标的含义](#user-content-9.目标检测中ap，ap50，ap75，map等指标的含义)
- [10.YOLOv2中的anchor如何生成？](#user-content-10.yolov2中的anchor如何生成？)
- [11.介绍下YOLOv5中的CSP结构？](#11.介绍下YOLOv5中的CSP结构？)
- [12.介绍下YOLOv5中的SPP结构?](#12.介绍下YOLOv5中的SPP结构?)


<h2 id="1.目标检测中iou的相关概念与计算">1.目标检测中IOU的相关概念与计算</h2>

IoU（Intersection over Union）即交并比，是目标检测任务中一个重要的模块，其是<font color=DeepSkyBlue>GT bbox与pred bbox交集的面积 / 二者并集的面积</font>。

![](https://files.mdnice.com/user/33499/a64f3919-f48d-402b-ad0f-8b61dd5c96eb.png)

下面我们用坐标（top，left，bottom，right），即左上角坐标，右下角坐标。从而可以在给定的两个矩形中计算IOU值。

```
def compute_iou(rect1,rect2):
  # (y0,x0,y1,x1) = (top,left,bottom,right)
  S_rect1 = (rect1[2] - rect1[0]) * (rect1[3] - rect1[1])
  S_rect2 = (rect2[2] - rect2[0]) * (rect2[3] - rect1[1])

  sum_all = S_rect1 + S_rect2
  left_line = max(rect1[1],rect2[1])
  right_line = min(rect1[3],rect2[3])
  top_line = max(rect1[0],rect2[0])
  bottom_line = min(rect1[2],rect2[2])

  if left_line >= right_line or top_line >= bottom_line:
    return 0
  else:
    intersect = (right_line - left_line) * (bottom_line - top_line)
    return (intersect / (sum_area - intersect)) * 1.0
```


<h2 id="2.目标检测中nms的相关概念与计算">2.目标检测中NMS的相关概念与计算</h2>

在目标检测中，我们可以利用非极大值抑制（NMS）对生成的大量候选框进行后处理，去除冗余的候选框，得到最具代表性的结果，以加快目标检测的效率。

如下图所示，消除多余的候选框，找到最佳的bbox：

![](https://files.mdnice.com/user/33499/816778a5-d6d3-42ee-bcf2-1322ee82f36a.png)

<font color=DeepSkyBlue>非极大值抑制（NMS）流程：</font>

1. 首先我们需要设置两个值：一个Score的阈值，一个IOU的阈值。

2. 对于每类对象，遍历该类的所有候选框，过滤掉Score值低于Score阈值的候选框，并根据候选框的类别分类概率进行排序：$A < B < C < D < E < F$。

3. 先标记最大概率矩形框F是我们要保留下来的候选框。

4. 从最大概率矩形框F开始，分别判断A～E与F的交并比（IOU）是否大于IOU的阈值，假设B、D与F的重叠度超过IOU阈值，那么就去除B、D。

5. 从剩下的矩形框A、C、E中，选择概率最大的E，标记为要保留下来的候选框，然后判断E与A、C的重叠度，去除重叠度超过设定阈值的矩形框。

6. 就这样重复下去，直到剩下的矩形框没有了，并标记所有要保留下来的矩形框。

7. 每一类处理完毕后，返回步骤二重新处理下一类对象。

```
import numpy as np

def py_cpu_nms(dets, thresh):
  #x1、y1（左下角坐标）、x2、y2（右上角坐标）以及score的值
  x1 = dets[:, 0]
  y1 = dets[:, 1]
  x2 = dets[:, 2]
  y2 = dets[:, 3]
  scores = dets[:, 4]

  #每一个候选框的面积
  areas = (x2 - x1 + 1) * (y2 - y1 + 1)
  #按照score降序排序（保存的是索引）
  order = scores.argsort()[::-1]

  keep = []
  while order.size > 0:
    i = order[0]
    keep.append(i)
    #计算当前概率最大矩形框与其他矩形框的相交框的坐标，会用到numpy的broadcast机制，得到向量
    xx1 = np.maximum(x1[i], x1[order[1:]])
    yy1 = np.maximum(y1[i], y1[order[1:]])
    xx2 = np.minimum(x2[i], x2[order[1:]])
    yy2 = np.minimum(y2[i], y2[order[1:]])

    #计算相交框的面积，注意矩形框不相交时w或h算出来会是负数，用0代替
    w = np.maximum(0.0, xx2 - xx1 + 1)
    h = np.maximum(0.0, yy2 - yy1 + 1)
    inter = w * h
    #计算重叠度IOU：重叠面积 / （面积1 + 面积2 - 重叠面积）
    ovr = inter / (areas[i] + areas[order[1:]] - inter)

    #找到重叠度不高于阈值的矩形框索引
    inds = np.where(ovr < thresh)[0]
    # 将order序列更新，由于前面得到的矩形框索引要比矩形框在原order序列中的索引小1，所以要加1操作
    order = order[inds + 1]

  return keep
```


<h2 id="3.one-stage目标检测与two-stage目标检测的区别？">3.One-stage目标检测与Two-stage目标检测的区别？</h2>

<font color=DeepSkyBlue>Two-stage目标检测算法</font>：先进行区域生成（region proposal，RP）（一个有可能包含待检物体的预选框），再通过卷积神经网络进行样本分类。其精度较高，速度较慢。

主要逻辑：特征提取—>生成RP—>分类/定位回归。

常见的Two-stage目标检测算法有：Faster R-CNN系列和R-FCN等。

<font color=DeepSkyBlue>One-stage目标检测算法</font>：不用RP，直接在网络中提取特征来预测物体分类和位置。其速度较快，精度比起Two-stage算法稍低。

主要逻辑：特征提取—>分类/定位回归。

常见的One-stage目标检测算法有：YOLO系列、SSD和RetinaNet等。

![](https://files.mdnice.com/user/33499/83c40e2f-8eef-4a2d-950a-22fe8c88c42c.png)

![](https://files.mdnice.com/user/33499/ce5bac5c-795e-4867-bdd4-93694bcd3e90.png)


<h2 id="4.哪些方法可以提升小目标检测的效果？">4.哪些方法可以提升小目标检测的效果？</h2>

1. 提高图像分辨率。小目标在边界框中可能只包含几个像素，那么能通过提高图像的分辨率以增加小目标的特征的丰富度。
  
2. 提高模型的输入分辨率。这是一个效果较好的通用方法，但是会带来模型inference速度变慢的问题。
  
3. 平铺图像。

![](https://files.mdnice.com/user/33499/21ddfa7a-4b39-4c1f-80f2-1907cbd6241c.png)

4. 数据增强。小目标检测增强包括随机裁剪、随机旋转和镶嵌增强等。
  
5. 自动学习anchor。
  
6. 类别优化。


<h2 id="5.focal-loss的作用？">5.Focal Loss的作用？</h2>

<font color=DeepSkyBlue>Focal Loss是解决了分类问题中类别不均衡、分类难度差异的一个损失函数，使得模型在训练过程中更加聚焦在困难样本上。</font>

Focal Loss是从二分类问题出发，同样的思想可以迁移到多分类问题上。

我们知道二分类问题的标准loss是**交叉熵**：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200621152403600.png)
对于二分类问题我们也几乎适用sigmoid激活函数$\hat{y} = \sigma(x)$，所以上面的式子可以转化成：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200621153119391.png)
这里有$1 - \sigma(x) = \sigma(-x)$。

Focal Loss论文中给出的式子如下：

![](https://img-blog.csdnimg.cn/20200621165614147.png#pic_center)

其中$y\in \{ 1,-1\}$是真实标签，$p\in[0,1]$是预测概率。

我们再定义$p_{t}:$

![](https://img-blog.csdnimg.cn/20200621170205510.png#pic_center)

那么，上面的交叉熵的式子可以转换成：

![](https://img-blog.csdnimg.cn/20200621170257779.png#pic_center)

有了上面的铺垫，最初Focal Loss论文中接着引入了**均衡交叉熵函数**：

![](https://img-blog.csdnimg.cn/20200621170521537.png#pic_center)

针对类别不均衡问题，在Loss里加入一个控制权重，对于属于少数类别的样本，增大$\alpha_{t}$即可。<font color=DeepSkyBlue>但这样有一个问题，它仅仅解决了正负样本之间的平衡问题，并没有区分易分/难分样本</font>。

**为什么上述公式只解决正负样本不均衡问题呢？**

因为增加了一个系数$\alpha_{t}$，跟$p_{t}$的定义类似，当$label=1$的时候$\alpha_{t}=\alpha$ ;当$label=-1$的时候，$\alpha_{t}= 1 - \alpha$，$\alpha$的范围也是$[0,1]$。因此可以通过设定$\alpha$的值（如果$1$这个类别的样本数比$-1$这个类别的样本数少很多，那么$\alpha$可以取$0.5$到$1$来增加$1$这个类的样本的权重）来控制正负样本对整体Loss的贡献。

<h3 id="focal-loss">Focal Loss</h3>

为了可以区分难/易样本，Focal Loss雏形就出现了：

![](https://img-blog.csdnimg.cn/20200621171619978.png#pic_center)

$(1 - p_{t})^{\gamma}$用于平衡难易样本的比例不均，$\gamma >0$起到了对$(1 - p_{t})$的放大作用。$\gamma >0$减少易分样本的损失，使模型更关注于困难易错分的样本。例如当$\gamma =2$时，模型对于某正样本预测置信度$p_{t}$为$0.9$，这时$(1 - 0.9)^{\gamma} = 0.01$，也就是FL值变得很小；而当模型对于某正样本预测置信度$p_{t}$为0.3时，$(1 - 0.3)^{\gamma} = 0.49$，此时它对Loss的贡献就变大了。当$\gamma = 0$时变成交叉熵损失。

为了应对正负样本不均衡的问题，在上面的式子中再加入平衡交叉熵的$\alpha_{t}$因子，用来平衡正负样本的比例不均，最终得到Focal Loss：

![](https://img-blog.csdnimg.cn/20200621163522857.png)

Focal Loss论文中给出的实验最佳取值为$a_{t}= 0.25$，$\gamma = 2$。

![](https://img-blog.csdnimg.cn/20200621164522236.png)


<h2 id="6.yolo系列的面试问题">6.YOLO系列的面试问题</h2>

Rocky之前总结了YOLOv1-v7全系列的解析文章，帮助大家应对可能出现的与YOLO相关的面试问题，大家可按需取用：

[【Make YOLO Great Again】YOLOv1-v7全系列大解析（汇总篇）](https://mp.weixin.qq.com/s?__biz=Mzg4NDYwOTUwNA==&mid=2247486017&idx=1&sn=ff028fa235f384fe74576ef346849600&chksm=cfb4d2cef8c35bd8726cdd3bc9664f2bc5edd1eaf3876e6ff60e523277461b43a57389621bc8&token=1199271157&lang=zh_CN#rd)


<h2 id="7.有哪些经典的轻量型人脸检测模型？">7.有哪些经典的轻量型人脸检测模型？</h2>

人脸检测相对于通用目标检测来说，算是一个子任务。比起通用目标检测任务动辄检测1000个类别，人脸检测任务主要聚焦于人脸的单类目标检测，<font color=DeepSkyBlue>使用通用目标检测模型太过奢侈，有点“杀鸡用牛刀”的感觉，并且大量的参数冗余，会影响部署侧的实用性</font>，故针对人脸检测任务，学术界提出了很多轻量型的人脸检测模型，Rocky在这里给大家介绍一些比较有代表性的：

1. libfacedetection
2. Ultra-Light-Fast-Generic-Face-Detector-1MB
3. A-Light-and-Fast-Face-Detector-for-Edge-Devices
4. CenterFace
5. DBFace
6. RetinaFace
7. MTCNN


<h2 id="8.lffd人脸检测模型的结构和特点？">8.LFFD人脸检测模型的结构和特点？</h2>

<font color=DeepSkyBlue>Rocky在实习/校招面试中被多次问到LFFD模型以及面试官想套取LFFD相关算法方案的情况，说明LFFD模型在工业界还是比较有价值的</font>，下面Rocky就带着大家学习一下LFFD模型的知识：

LFFD（A-Light-and-Fast-Face-Detector-for-Edge-Devices）适用于人脸、行人、车辆等单目标检测任务，具有速度快，模型小，效果好的特点。<font color=DeepSkyBlue>LFFD是Anchor-free的方法，使用感受野替代Anchors，并在主干结构上抽取8路特征图对从小到大的人脸进行检测，检测模块分为类别二分类与边界框回归</font>。

**LFFD模型结构**

![](https://img-blog.csdnimg.cn/20200628150256899.png)

我们可以看到，LFFD模型主要由四部分组成：tiny part、small part、medium part、large part。

模型中并没有采用BN层，因为BN层会减慢17%的推理速度。其主要采用尽可能快的下采样来保持100%的人脸覆盖。

**LFFD主要特点：**

1. 结构简单直接，易于在主流AI端侧设备中进行部署。

2. 检测小目标能力突出，在极高分辨率（比如8K或更大）画面，可以检测其间10个像素大小的目标；

**LFFD损失函数**

LFFD损失函数是由regression loss和classification loss的加权和。

分类损失使用了交叉熵损失。

回归损失使用了L2损失函数。

LFFD论文地址：[LFFD: A Light and Fast Face Detector for Edge Devices论文地址](https://arxiv.org/pdf/1904.10633.pdf)


<h2 id="9.目标检测中ap，ap50，ap75，map等指标的含义">9.目标检测中AP，AP50，AP75，mAP等指标的含义</h2>

AP：PR曲线下的面积。

![PR曲线](https://files.mdnice.com/user/33499/2bebf6d5-3270-4e8e-9a66-050678c312ea.png)

AP50: 固定IoU为50%时的AP值。

AP75:固定IoU为75%时的AP值。

AP@[0.5:0.95]:把IoU的值从50%到95%每隔5%进行了一次划分，并对这10组AP值取平均。

mAP：对所有的类别进行AP的计算，然后取均值。

mAP@[.5:.95]（即mAP@[.5,.95]）：表示在不同IoU阈值（从0.5到0.95，步长0.05）（0.5、0.55、0.6、0.65、0.7、0.75、0.8、0.85、0.9、0.95）上的平均mAP。


<h2 id="10.yolov2中的anchor如何生成？">10.YOLOv2中的anchor如何生成？</h2>

YOLOv2中<font color=DeepSkyBlue>引入K-means算法进行anchor的生成</font>，可以自动找到更好的anchor宽高的值用于模型训练的初始化。

但如果使用经典K-means中的欧氏距离作为度量，意味着较大的Anchor会比较小的Anchor产生更大的误差，聚类结果可能会偏离。

由于目标检测中主要关心anchor与ground true box（gt box）的IOU，不关心两者的大小。因此，使用IOU作为度量更加合适，即提高IOU值。因此YOLOv2采用IOU值为评判标准：

$$d(gt box,anchor) = 1 - IOU(gt box,anchor)$$

具体anchor生成步骤与经典K-means大致相同，在下一个章节中会详细介绍。主要的不同是使用的度量是$d(gt box,anchor)$，并将anchor作为簇的中心。


<h2 id="11.介绍下YOLOv5中的CSP结构？">11.介绍下YOLOv5中的CSP结构？</h2>
CSP(Cross Stage Parial Network)结构是YOLOv5中Backbone网络中用来提高特征识别能力的技术。
它可以提供更高级别的特征，加快特征提取的速度，减少模型参数量，从而使YOLOv5能够更好地进行目标检测。

CSP结构的主要思想是将输入特征图分成两个分支，每个分支都包含一些卷积层和残差连接，然后通过一个跨阶段的连接将两个分支组合起来。

具体来说，CSP结构包含以下几个主要组成部分:
- **块内部残差连接(Residual Connection)**：每个块内部都包含一个残差连接，用于提高信息传递和反向梯度流的效果。
具体来说，每个块内部的第一个卷积层输出的特征图会被直接添加到块的最后一个卷积层输出的特征图上。
- **跨阶段连接(Cross-Stage Connection)**：CSP结构通过一个跨阶段的连接将两个分支组合起来。具体来说，将输入特征图分成两个相等的部分， 
其中一部分作为主干部分，另一部分作为侧分支。然后在侧分支上应用一些卷积层，将其输出与主干部分的输出串联起来，得到最终的特征图输出。
- **块内部的卷积层和批归一化(Convolution and Batch Normalization)**：每个块内部包含一些卷积层和批归一化层，用于提取特征和规范化特征图。
具体来说，每个块内部的第一个卷积层通常是一个3x3的卷积层，其后跟一个批归一化层和ReLU激活函数。
然后是一系列1x1的卷积层和3x3的卷积层，每个卷积层后都跟着批归一化层和ReLU激活函数.
- **块之间的下采样(Downsampling)**：CSP结构中的每个块之间都包含一个下采样步骤，用于将特征图分辨率减半。具体来说，下采
样通常通过一个步长为2的卷积层实现，也可以通过池化层来实现。

>![输入图片描述](imgs/csp.png)

源码分析:
```python
class BottleneckCSP(nn.Module):
    #CSP结构
    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, number, shortcut, groups, expansion
        super().__init__()
        c_ = int(c2 * e)  # hidden channels
        self.cv1 = Conv(c1, c_, 1, 1)#对应上面网络结构图的上面的分支的第一个CBL
        self.cv2 = nn.Conv2d(c1, c_, 1, 1, bias=False)#对应上面网络结构图的下面的分支的conv
        self.cv3 = nn.Conv2d(c_, c_, 1, 1, bias=False)#对应上面网络结构图的上面的分支的conv
        self.cv4 = Conv(2 * c_, c2, 1, 1)#对应最后的CBL
        self.bn = nn.BatchNorm2d(2 * c_)  # applied to cat(cv2, cv3)
        self.act = nn.SiLU()#对应Concat后的Leaky ReLU
        self.m = nn.Sequential(*(Bottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n)))
        #nn.Sequential--序贯模型是函数式模型的简略版，为最简单的线性、从头到尾的结构顺序，不分叉，是多个网络层的线性堆叠。
        #self.m对应X个Resunit or 2 * X个CBL（对应的切换是通过Bottleneck类中的True 或 False决定，True为X个Resunit，False为2 * X个CBL）
    def forward(self, x):
        y1 = self.cv3(self.m(self.cv1(x)))#对应上面网络结构图的上面的分支
        y2 = self.cv2(x)#对应上面网络结构图的下面的分支
        return self.cv4(self.act(self.bn(torch.cat((y1, y2), dim=1))))
        #torch.cat对应Concat
        #self.bn对应Concat后的BN
```


<h1 id="12.介绍下YOLOv5中的SPP结构?">12.介绍下YOLOv5中的SPP结构?</h1>

### SPP应用的背景：

在卷积神经网络中我们经常看到固定输入的设计，但是如果输入不能是固定尺寸的该怎么办呢？

通常来说，有以下几种方法：

（1）对输入进行resize操作，让他们统统变成你设计的层的输入规格那样。但是这样过于暴力直接，可能会丢失很多信息或者多出很多不该有的信息（图片变形等），影响最终的结果。

（2）替换网络中的全连接层，对最后的卷积层使用global average pooling，全局平均池化只和通道数有关，而与特征图大小没有关系

（3）最后一个当然是我们要讲的SPP结构啦~

### SPP结构分析

SPP结构又被称为空间金字塔池化，能将任意大小的特征图转换成固定大小的特征向量。

**处理流程：**

- 输入层：首先我们现在有一张任意大小的图片，其大小为w * h。

- 输出层：21个神经元 -- 即我们希望提取到21个特征。

分析如下图所示：分别对1 * 1分块，2 * 2分块和4 * 4子图里分别取每一个框内的max值（即取蓝框框内的最大值），
这一步就是作最大池化，这样最后提取出来的特征值（即取出来的最大值）一共有1 * 1 + 2 * 2 + 4 * 4 = 21个。得出的特征再concat在一起。
>![输入图片描述](imgs/spp1.png)

在YOLOv5中SPP的结构图如下图所示：

>![输入图片描述](imgs/spp2.png)
